{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating VPIN CDF for Banking Stocks: JPM, BAC, C, USB, WFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd   \n",
    "from IPython.core.debugger import Tracer\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to aggregate the trading volume into volume buckets and derive the Buy and Sell volume based on return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_buckets(df,bucketSize):\n",
    "    volumeBuckets = pd.DataFrame(columns=['Buy','Sell','Time'])\n",
    "    count = 0\n",
    "    BV = 0\n",
    "    SV = 0\n",
    "    for index,row in df.iterrows():\n",
    "        newVolume = row['volume']\n",
    "        z = row['z']\n",
    "        if bucketSize < count + newVolume:\n",
    "            BV = BV + (bucketSize-count)*z\n",
    "            SV = SV + (bucketSize-count)*(1-z)\n",
    "            volumeBuckets = volumeBuckets.append({'Buy':BV, 'Sell':SV, 'Time':index},ignore_index=True)\n",
    "            count = newVolume-(bucketSize-count)\n",
    "            if int(count/bucketSize) > 0:\n",
    "                for i in range(0,int(count/bucketSize)):\n",
    "                    BV = (bucketSize)*z\n",
    "                    SV = (bucketSize)*(1-z)\n",
    "                    volumeBuckets = volumeBuckets.append({'Buy':BV, 'Sell':SV, 'Time':index},ignore_index=True)\n",
    "\n",
    "            count = count%bucketSize\n",
    "            BV = (count)*z\n",
    "            SV = (count)*(1-z)\n",
    "        else:\n",
    "            BV = BV + (newVolume)*z\n",
    "            SV = SV + (newVolume)*(1-z)\n",
    "            count = count + newVolume\n",
    "\n",
    "    volumeBuckets = volumeBuckets.set_index('Time')\n",
    "    return volumeBuckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate the VPIN and CDF metrics from Buy and Sell Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_vpin(data, bucketSize,window):\n",
    "    \n",
    "    volume = (data['SIZE'])\n",
    "    trades = (data['PRICE'])\n",
    "    \n",
    "    trades_1min = trades.diff(1).resample('1min', how='sum').dropna()\n",
    "    volume_1min = volume.resample('1min', how='sum').dropna()\n",
    "    sigma = trades_1min.std()\n",
    "    z = trades_1min.apply(lambda x: norm.cdf(x/sigma))\n",
    "    df = pd.DataFrame({'z': z, 'volume': volume_1min}).dropna()\n",
    "    \n",
    "    volumeBuckets=get_buckets(df,bucketSize)\n",
    "    volumeBuckets['VPIN'] = abs(volumeBuckets['Buy']-volumeBuckets['Sell']).rolling(window).mean()/bucketSize\n",
    "    volumeBuckets['CDF'] = volumeBuckets['VPIN'].rank(pct=True)\n",
    "    \n",
    "    return volumeBuckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading trade date of the specific stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sym = ['C','BAC','USB','JPM','WFC']\n",
    "df={}; sec_trades = {}\n",
    "for s in sym:\n",
    "    f = s + '.csv'\n",
    "    print('Reading '+f)\n",
    "    sec_trades[s] = pd.read_csv(f,parse_dates=[['DATE','TIME_M']],index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting bucket size and calculating VPINs and CDFs for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "volume = {'C':300000,'USB':100000,'JPM':200000,'BAC':1250000,'WFC':300000}\n",
    "for s in sym:\n",
    "    print('Calculating VPIN')\n",
    "    df[s] = calc_vpin(sec_trades[s],volume[s],50)\n",
    "    print(s+' '+str(df[s].shape))\n",
    "    \n",
    "avg = pd.DataFrame()\n",
    "print(avg.shape)\n",
    "metric = 'CDF'\n",
    "avg[metric] = np.nan\n",
    "for stock,frame in df.items():\n",
    "    frame = frame[[metric]].reset_index().drop_duplicates(subset='Time', keep='last').set_index('Time')\n",
    "    avg = avg.merge(frame[[metric]],left_index=True,right_index=True,how='outer',suffixes=('',stock))\n",
    "    print(avg.shape)\n",
    "avg = avg.dropna(axis=0,how='all').fillna(method='ffill')\n",
    "\n",
    "avg.to_csv('CDF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the average rolling correlation between VPIN CDF of banking stocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = ['Time','CDFC','CDFBAC','CDFUSB','CDFJPM','CDFWFC']\n",
    "df = pd.read_csv('CDF.csv',parse_dates=['Time'],index_col=[0],usecols = fields)\n",
    "\n",
    "rolling_pariwise_corr = pd.rolling_corr(df,window=50,pairwise=True)\n",
    "\n",
    "thres = pd.DataFrame()\n",
    "thres['AvgCorrAssets'] = rolling_pariwise_corr.groupby(by=['Time']).sum().sum(axis=1)/((len(fields)-1)**2)\n",
    "thres.to_csv('AvgCorrAssets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating VPIN CDF for BAC on individual exchanges and deriving the average rolling correlations between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating VPIN for each exchange based on different buckets taking average daily volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec_trades = pandas.read_csv('BAC.csv',parse_dates=[['DATE','TIME_M']],index_col=[0])\n",
    "print('File read complete')\n",
    "\n",
    "exchanges = sec_trades['EX'].unique()\n",
    "exchanges = exchanges[:-2]\n",
    "bucketsize_standard = 100000\n",
    "rolling_window = 50\n",
    "df_list = list()\n",
    "df_vpin_list = list()\n",
    "volume_exchanges = list()\n",
    "bucketsize = list()\n",
    "for i in range(len(exchanges)-1):\n",
    "    df_list.append(sec_trades[sec_trades['EX'] == exchanges[i]])\n",
    "for i in range(len(exchanges)):\n",
    "    volume_exchanges.append(df_list[i]['SIZE'].sum())\n",
    "nbuckets = 6574\n",
    "for i in range(len(exchanges)):\n",
    "    bucketsize = int(volume_exchanges[i]/nbuckets)\n",
    "    df_vpin_list.append(calc_vpin(df_list[i],bucketsize,rolling_window))\n",
    "    \n",
    "    \n",
    "avg = pd.DataFrame()\n",
    "metric = 'VPIN'\n",
    "avg[metric] = np.nan\n",
    "for i in range(len(exchanges)):\n",
    "    print(exchanges[i])\n",
    "    frame = df_vpin_list[i]\n",
    "    frame = frame[[metric]].reset_index().drop_duplicates(subset='Time', keep='last').set_index('Time')\n",
    "    avg = avg.merge(frame[[metric]],left_index=True,right_index=True,how='outer',suffixes=('',exchanges[i]))\n",
    "    print(avg.shape)\n",
    "avg = avg.dropna(axis=0,how='all').fillna(method='ffill')\n",
    "del avg['VPIN']\n",
    "avg = avg.dropna()\n",
    "print(avg)\n",
    "\n",
    "rolling_pariwise_corr = pd.rolling_corr(avg,window=50,pairwise=True)\n",
    "thres = pd.DataFrame()\n",
    "thres['AvgCorrEx'] = rolling_pariwise_corr.groupby(by=['Time']).sum().sum(axis=1)/(len(exchanges)**2)\n",
    "print(thres.tail())\n",
    "thres.to_csv('AvgCorrEx.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Quote Imbalance based on Quotes data for BAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read and clean BAC quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(x):\n",
    "    x=x[x['SYM_SUFFIX'].isnull()]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sec_quotes = pandas.read_csv('BAC_QF.csv',parse_dates=[['DATE','TIME_M']],index_col=[0])\n",
    "sec_quotes=cleanup(sec_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quote Imbalance function which calculates the average difference in volume of bid quotes and ask quotes among all exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imbalance(sec_quotes):\n",
    "    bids={}\n",
    "    bids_vol={}\n",
    "    asks={}\n",
    "    asks_vol={}\n",
    "    sec_bids=sec_quotes[sec_quotes['BID']>0]\n",
    "    sec_bids=sec_bids[sec_bids['BIDSIZ']>0]\n",
    "    sec_asks=sec_quotes[sec_quotes['ASK']>0]\n",
    "    sec_asks=sec_asks[sec_quotes['ASKSIZ']>0]\n",
    "    for ex in sec_quotes['EX'].unique():\n",
    "        bids[ex]=(sec_bids[sec_bids['EX']==ex]['BID'])\n",
    "        bids_vol[ex]=(sec_bids[sec_bids['EX']==ex]['BIDSIZ'])\n",
    "\n",
    "        asks[ex]=(sec_asks[sec_asks['EX']==ex]['ASK'])\n",
    "        asks_vol[ex]=(sec_asks[sec_asks['EX']==ex]['ASKSIZ'])\n",
    "\n",
    "    \n",
    "    df_comb=pd.DataFrame()\n",
    "    for ex in sec_quotes['EX'].unique():\n",
    "        df=pd.DataFrame()\n",
    "        df1=pd.DataFrame()\n",
    "        bidquote_1min = bids[ex].resample('1min', how='last').ffill().fillna(0)\n",
    "        bidvol_1min = bids_vol[ex].resample('1min', how='last').ffill().fillna(0)\n",
    "        askquote_1min = asks[ex].resample('1min', how='last').ffill().fillna(0)\n",
    "        askvol_1min = asks_vol[ex].resample('1min', how='last').ffill().fillna(0)\n",
    "        df1=pd.concat([bidquote_1min, bidvol_1min,askquote_1min,askvol_1min], join='outer', axis=1)\n",
    "        df1=df1.ffill().fillna(0)\n",
    "        df['bprice_'+ex]=df1['BID']\n",
    "        df['bvol_'+ex]=df1['BIDSIZ']\n",
    "        df['aprice_'+ex]=df1['ASK']\n",
    "        df['avol_'+ex]=df1['ASKSIZ']\n",
    "\n",
    "        if df_comb.empty:\n",
    "            df_comb=df.copy()\n",
    "            df_comb['avg_bid']=df['bprice_'+ex]*df['bvol_'+ex]\n",
    "            df_comb['price_bid']=df['bprice_'+ex]\n",
    "            df_comb['avg_ask']=df['aprice_'+ex]*df['avol_'+ex]\n",
    "            df_comb['price_ask']=df['aprice_'+ex]\n",
    "            n1=df_comb['bprice_'+ex].apply(lambda x: int(x!=0))\n",
    "            n2=df_comb['aprice_'+ex].apply(lambda x: int(x!=0))\n",
    "        else:\n",
    "            df_comb=df_comb.merge(df, how='outer', right_index=True, left_index=True).ffill().fillna(0)\n",
    "            df_comb['avg_bid']=df_comb['avg_bid']+df_comb['bprice_'+ex]*df_comb['bvol_'+ex]\n",
    "            df_comb['price_bid']=df_comb['price_bid']+df_comb['bprice_'+ex]\n",
    "            df_comb['avg_ask']=df_comb['avg_ask']+df_comb['aprice_'+ex]*df_comb['avol_'+ex]\n",
    "            df_comb['price_ask']=df_comb['price_ask']+df_comb['aprice_'+ex]\n",
    "            n1=n1+df_comb['bprice_'+ex].apply(lambda x: int(x!=0))\n",
    "            n2=n2+df_comb['aprice_'+ex].apply(lambda x: int(x!=0))\n",
    "\n",
    "    df_comb['avg_bid']=df_comb['avg_bid']*n1/df_comb['price_bid']\n",
    "    df_comb['avg_ask']=df_comb['avg_ask']*n2/df_comb['price_ask']\n",
    "    imbalance=df_comb['avg_bid']-df_comb['avg_ask']\n",
    "    return imbalance\n",
    "\n",
    "quote_imb=imbalance(sec_quotes)\n",
    "quote_imb.to_csv('BACquote_imb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Theory Based and ML Based Trading Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and processing data for Trading Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_corr_assets = pd.read_csv('req_files/AvgCorrAssets.csv',parse_dates=['Time'],index_col='Time')\n",
    "df_corr_ex = pd.read_csv('req_files/AvgCorrEx.csv',parse_dates=['Time'],index_col='Time')\n",
    "df_vpin = pd.read_csv('req_files/BACVPIN.csv',parse_dates=['Time'],usecols=['Time','CDF'],index_col='Time')\n",
    "df_quote_imb = pd.read_csv('req_files/imbalance.csv',parse_dates=['TIME'],index_col='TIME')\n",
    "df_price = pd.read_csv('req_files/BACprice.csv',parse_dates=['DATE_TIME_M'],index_col='DATE_TIME_M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_df = pd.DataFrame()\n",
    "frame = df_corr_assets[[df_corr_assets.columns[0]]].reset_index().drop_duplicates(subset='Time', keep='last').set_index('Time')\n",
    "total_df = total_df.merge(frame[[frame.columns[0]]],left_index=True,right_index=True,how='outer')\n",
    "print (total_df.shape)\n",
    "\n",
    "frame = df_corr_ex[[df_corr_ex.columns[0]]].reset_index().drop_duplicates(subset='Time', keep='last').set_index('Time')\n",
    "total_df = total_df.merge(frame[[frame.columns[0]]],left_index=True,right_index=True,how='outer')\n",
    "print (total_df.shape)\n",
    "\n",
    "frame = df_vpin[[df_vpin.columns[0]]].reset_index().drop_duplicates(subset='Time', keep='last').set_index('Time')\n",
    "total_df = total_df.merge(frame[[frame.columns[0]]],left_index=True,right_index=True,how='outer')\n",
    "print (total_df.shape)\n",
    "\n",
    "frame = df_quote_imb[[df_quote_imb.columns[0]]].reset_index().drop_duplicates(subset='TIME', keep='last').set_index('TIME')\n",
    "total_df = total_df.merge(frame[[frame.columns[0]]],left_index=True,right_index=True,how='outer')\n",
    "print (total_df.shape)\n",
    "\n",
    "frame = df_price[[df_price.columns[0]]].reset_index().drop_duplicates(subset='DATE_TIME_M', keep='last').set_index('DATE_TIME_M')\n",
    "total_df = total_df.merge(frame[[frame.columns[0]]],left_index=True,right_index=True,how='outer')\n",
    "print (total_df.shape)\n",
    "\n",
    "total_df = total_df.dropna(axis=0,how='all').fillna(method='ffill')\n",
    "total_df = total_df.dropna(how='any')\n",
    "print (total_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_df.to_csv('data_qlearner.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('data_qlearner_raw.csv', parse_dates=['Time'],index_col='Time' )\n",
    "df=df[df['zcorras'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['returns']=df['PRICE'].pct_change()\n",
    "df['returns']=df['returns'].shift(-1)\n",
    "df=df.replace([np.inf, -np.inf], np.nan)\n",
    "df=df.ffill()\n",
    "df = df[:-1]\n",
    "df=df.between_time('8:00','16:00')\n",
    "df=df[df.index.dayofweek < 5]\n",
    "df=df[df.index<pd.to_datetime('2016-12-30')]\n",
    "df.to_csv('new_data.csv')\n",
    "\n",
    "savg=df[['AvgCorrEx','AvgCorrAssets','quote_imb']].rolling(window=100).mean()\n",
    "sstd=df[['AvgCorrEx','AvgCorrAssets','quote_imb']].rolling(window=100).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating z-scores for input features using rolling mean and standard deviation; and windsoring z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zcorrex=(df['AvgCorrEx']-savg['AvgCorrEx'])/sstd['AvgCorrEx']\n",
    "zcorras=(df['AvgCorrAssets']-savg['AvgCorrAssets'])/sstd['AvgCorrAssets']\n",
    "zimb=(df['quote_imb']-savg['quote_imb'])/sstd['quote_imb']\n",
    "\n",
    "zcorrex[zcorrex>3]=3\n",
    "zcorrex[zcorrex<-3]=-3\n",
    "zcorras[zcorras>3]=3\n",
    "zcorras[zcorras<-3]=-3\n",
    "zimb[zimb>3]=3\n",
    "zimb[zimb<-3]=-3\n",
    "\n",
    "\n",
    "df_trading=pd.DataFrame({'price':df['PRICE'][100:],'cdf':df['CDF'][100:],'zcorrex':zcorrex[100:],'zcorras':zcorras[100:],'zimb':zimb[100:],'returns':df['returns'][100:], 'Time':df.index[100:]}\n",
    "                        ).set_index('Time')\n",
    "df_trading.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the theory based strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos=pd.DataFrame(columns=['position','returns','Time'])\n",
    "position=0\n",
    "\n",
    "for index,row in df_trading.iterrows():\n",
    "    if(row['cdf']>0.5 and row['zcorrex']+row['zcorras']>4):\n",
    "        if(row['zimb']>1.5):\n",
    "            if(position<0):\n",
    "                position=0\n",
    "            position+=0.1\n",
    "            bcount=10\n",
    "        if(row['zimb']<-1.5):\n",
    "            if(position>0):\n",
    "                position=0\n",
    "            position-=0.1\n",
    "            scount=10\n",
    "    if(position>0):\n",
    "        bcount=bcount-1\n",
    "        if(bcount==0):\n",
    "            position=0\n",
    "    elif(position<0):\n",
    "        scount=scount-1\n",
    "        if(scount==0):\n",
    "            position=0\n",
    "    pos=pos.append({'position':position,'returns':row['returns'], 'price':row['price'],'Time':index},ignore_index=True)\n",
    "    #print('position :'+ str(position))\n",
    "pos=pos.set_index('Time')\n",
    "pos['wealth']=(1+pos['returns']*pos['position']).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax=pos[['position','wealth']].plot(secondary_y=['wealth'])\n",
    "ax.figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying ML Classification Based Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv('final.csv', parse_dates=['Time'],index_col='Time' )\n",
    "df=df[df['zcorras'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretizing returns data and creating Training/ Test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[['cdf','zcorras','zcorrex','zimb']]\n",
    "y=(df['returns']>0).astype(int)\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "df_X_train = X[:10000]\n",
    "df_X_test = X[10000:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "df_y_train = y[:10000]\n",
    "df_y_test = y[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the data to different classifiers and obtaining the detailed stats for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score=[]\n",
    "pos=None\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(4),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=0.25, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=4),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "for cla,name in zip(classifiers,names): \n",
    "    clf=cla\n",
    "    clf.fit(df_X_train, df_y_train)\n",
    "    score.append(accuracy_score(df_y_test,clf.predict(df_X_test)))\n",
    "    position=((clf.predict_proba(df_X_test))*[-0.05,0.05]).sum(axis=1)\n",
    "    print(classification_report(df_y_test, clf.predict(df_X_test)))\n",
    "    wealth=(1+df['returns'][10000:]*position).cumprod()\n",
    "    if(pos is None):\n",
    "        pos=pd.DataFrame({'returns':df['returns'][10000:], name:wealth,'price':df['price'][10000:],'Time':df.index[10000:]}).set_index('Time')\n",
    "    else:\n",
    "        pos[name]=position\n",
    "    ax=pos[[name,'price']].plot(secondary_y=[name])\n",
    "    ax.figure\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
